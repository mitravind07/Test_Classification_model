# README.md

# ğŸ“ Text Classification with DistilBERT

This project demonstrates how to build a text classification pipeline using a pre-trained transformer model from Hugging Face on the IMDB sentiment dataset. The pipeline includes tokenization, model fine-tuning, evaluation, and inference.

---

## ğŸ“ Folder Structure

```
project-root/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ eda_and_train.ipynb        # EDA + training notebook
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py             # Dataset loading (optional for scripting)
â”‚   â”œâ”€â”€ train.py                   # Training script
â”‚   â”œâ”€â”€ evaluate.py                # Evaluation script
â”‚   â””â”€â”€ inference.py               # Run predictions on custom input
â”œâ”€â”€ models/
â”‚   â””â”€â”€ distilbert/                # Fine-tuned model artifacts
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ submission.md              # Project summary and insights
â”œâ”€â”€ requirements.txt               # Required packages
â””â”€â”€ README.md                      # This file
```

---

## âš™ï¸ Setup Instructions

### 1. Create Virtual Environment (optional but recommended)
```bash
python -m venv venv
source venv/bin/activate  # For Linux/Mac
venv\Scripts\activate     # For Windows
```

### 2. Install Required Libraries
```bash
pip install -r requirements.txt
```

If `requirements.txt` is missing, you can manually install:
```bash
pip install torch transformers datasets scikit-learn
```

### 3. Download Dataset (IMDB)
No need to manually download; it's automatically fetched using `datasets.load_dataset("imdb")`.

---

## ğŸš€ Run the Project

### â¤ Option 1: Run All Steps in Notebook
Open and run `notebooks/eda_and_train.ipynb` to execute tokenization, training, evaluation, and inference in one place.

### â¤ Option 2: Use Python Scripts

#### 1. Train the Model
```bash
python src/train.py
```

#### 2. Evaluate the Model
```bash
python src/evaluate.py
```

#### 3. Make Predictions
```bash
python src/inference.py
```

---

## ğŸ§ª Evaluation
- Evaluation metrics include accuracy, precision, recall, and F1-score using `sklearn`.
- Results are printed after running `evaluate.py`.

---

## ğŸŒ Multilingual Extension (Bonus)
To make the project multilingual:
- Replace model: use `xlm-roberta-base`
- Use dataset: `amazon_reviews_multi`
- Update tokenizer and training logic accordingly

---

## ğŸ‘©â€ğŸ’» Author
ML/NLP Engineer Intern Project

---

## ğŸ“„ License
This project is for educational and internship purposes.

